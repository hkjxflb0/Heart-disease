# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13v4QerjkByZ4nNH8B1UtMJiQphA1GhPF
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

df = pd.read_csv('/content/HeartDiseaseTrain-Test - HeartDiseaseTrain-Test.csv (3).csv')

df.head()

df.shape

df.isnull().sum()

df.info()

df.describe()

df.rename(columns={"cholestoral": "cholesterol"}, inplace=True)

cat_col = [col for col in df.columns if df[col].dtype == 'object']

cat_col

num_col = [col for col in df.columns if df[col].dtype != 'object']

num_col

numerical_corr = {}
for col in num_col:
    if col == 'target':
      continue
    corr = df["target"].corr(df[col])
    print(col, corr)
    numerical_corr[col] = corr

plt.figure(figsize=(15,10))
sns.heatmap(df.corr(numeric_only=True), annot=True)

sort_corr = sorted(numerical_corr.items(), key=lambda ele:abs(ele[1]))

for i in sort_corr:
    print(f"{i[0]}:{abs(i[1])}")

plt.plot(numerical_corr.keys(),numerical_corr.values())
plt.xticks(rotation=90)
plt.show()

""" The oldpeak has the highest impact on heart disease whereas cholesterol has the lowest and max_heart rate also plays a significant role in this but it is still unknow about Good cholesterol? Bad cholesterol? perhaps a combination of both in a skewed ratio?"""

import plotly.express as px

"""First  I should check that by data is baised or not."""

df['target'].value_counts()

"""It is clear that dataset is not baised and we can proceed with the outlier detection in numerical features.

I found this plotly in kaggle which is a library to visualize the data in more interractive way, so i am going to use it here.
"""

template = "plotly_white"
# color_scale = "Bluered"
# color_discrete_sequence=["lightblue", "orangered"] # For discrete map visualization
num_data = df
# Convert target to string for discrete color representation
num_data["target"] = num_data["target"].astype(str)

fig1 = px.box(data_frame=num_data, x="cholesterol", y="target", template=template, title="Cholesterol vs Heart Disease",color=num_data['target'])
fig1.show();

fig2 = px.box(data_frame=df, x="Max_heart_rate", y="target", template=template, title="Max_heart_rate vs heart disease",color=num_data['target'])
fig2.show();

fig3 = px.box(data_frame=df, x="target", y="age", template=template, title="Age vs heart disease",color=num_data['target'])
fig3.show();

fig4 = px.box(data_frame=df, x="resting_blood_pressure", y="target", template=template, title="Resting blood pressure vs heart disease",color=num_data['target'])
fig4.show();

fig5 = px.box(data_frame=df, x="oldpeak", y="target", template=template, title="Oldpeak vs heart disease",color=num_data['target'])
fig5.show();

sex_map = {
    "Male": 1,
    "Female": 0
}
chest_pain_map = {
    "Typical angina": 1,
    "Atypical angina": 2,
    "Non-anginal pain": 3,
    "Asymptomatic": 4
}
blood_sugar_map = {
    "Lower than 120 mg/ml": 0,
    "Greater than 120 mg/ml": 1
}
rest_ecg_map = {
    "Normal": 0,
    "ST-T wave abnormality": 1,
    "Left ventricular hypertrophy": 2,
}
exercise_angina_map = {
    "Yes": 1,
    "No": 0
}
slope_map = {
    "Upsloping": 1,
    "Flat": 2,
    "Downsloping": 3
}
fluoroscopy_map = {
    "Zero": 0,
    "One": 1,
    "Two": 2,
    "Three": 3,
    "Four": 4
}
thalassemia_map = {
    "No": 0,
    "Normal": 3,
    "Fixed Defect": 6,
    "Reversable Defect": 7
}

df_cat = df

df_cat["sex"].replace(sex_map, inplace=True)
df_cat["chest_pain_type"].replace(chest_pain_map, inplace=True)
df_cat["fasting_blood_sugar"].replace(blood_sugar_map, inplace=True)
df_cat["rest_ecg"].replace(rest_ecg_map, inplace=True)
df_cat["exercise_induced_angina"].replace(exercise_angina_map, inplace=True)
df_cat["slope"].replace(slope_map, inplace=True)
df_cat["vessels_colored_by_flourosopy"].replace(fluoroscopy_map, inplace=True)
df_cat["thalassemia"].replace(thalassemia_map, inplace=True)

df_cat["target"] = df_cat["target"].astype(np.int64)

# Reset df_raw
df_raw = pd.read_csv("/content/HeartDiseaseTrain-Test - HeartDiseaseTrain-Test.csv (3).csv")
# Fix minor typo from Cholestoral to Cholesterol
df_raw.rename(columns={"cholestoral": "cholesterol"}, inplace=True)

cat_cols = list(df_raw.select_dtypes(include=object))
cat_corr = {}
for col in cat_cols:
    corr = df_cat["target"].corr(df_cat[col])
    cat_corr[col] = corr

# Sort by absolute impact
cat_corr_sorted = sorted(cat_corr.items(), key=lambda ele:abs(ele[1]))
cat_corr_sorted

plt.plot(cat_corr.keys(),cat_corr.values())
plt.xticks(rotation=90)
plt.show()

"""It can be clearly seen fasting blood sugar has the least impact on heart disease in this distribution whereas exercise induced angina has the most impact."""

df_cat_str = df_raw
for col in cat_cols:
    df_cat_str[col] = df_raw[col].astype(str)
df_cat_str["target"] = df_raw["target"]

px.histogram(data_frame=df_cat_str, x="fasting_blood_sugar", color="target" , template=template, title="Fasting blood sugar - Heart Disease 0: <120 mg/dl, 1: >120 mg/dl")

px.histogram(data_frame=df_cat_str, x="sex", color="target" , template=template, title="Sex & Heart Disease")

px.histogram(data_frame=df_cat_str, x="thalassemia", color="target" , template=template, title="Thalassemia vs Heart Disease")

px.histogram(data_frame=df_cat_str, x="slope", color="target" , template=template, title="Slope of peak exercise ST vs Heart Disease")

px.histogram(data_frame=df_cat_str, x="vessels_colored_by_flourosopy", color="target" , template=template, title="Vessels colored by flourosopy vs Heart Disease")

px.histogram(data_frame=df_cat_str, x="chest_pain_type", color="target" , template=template, title="Chest Pain Type vs Heart Disease")

px.histogram(data_frame=df_cat_str, x="exercise_induced_angina", color="target" , template=template, title="Exercise induced angina vs Heart Disease")

for col in cat_cols:
    df_cat[col] = df_cat[col].astype(int)

Results = {}

from sklearn.linear_model import LogisticRegression
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split

x = df.columns[:-1]
y = df.columns[-1]
x

y

x = df[x]
y = df[y]

x.head()

y.head()

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

x_train.shape

x_test.shape

y_train.shape

y_test.shape

logistic_reg = LogisticRegression(solver="liblinear")
logistic_reg.fit(x_train, y_train)
score_log = logistic_reg.score(x_test, y_test)
score_log

from sklearn.metrics import confusion_matrix

y_pred = logistic_reg.predict(x_test)

# Calculate the confusion matrix
cm = confusion_matrix(y_test, y_pred)
cm

"""As we can see with base model we are getting 85 accuracy, now lets apply hyperparameter tuning

"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Define the hyperparameters to tune
param_grid = {
    'C': [0.1, 1, 10, 100],
    'solver': ['liblinear', 'saga'],
    'penalty': ['l1', 'l2'],
    'max_iter': [100, 200, 500]
}

grid_search = GridSearchCV(logistic_reg, param_grid, cv=5, scoring='accuracy')
grid_search.fit(x_train, y_train)

best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Evaluate on test data
score_log = grid_search.score(x_test, y_test)

best_params, best_score, score_log

low_corr = ["cholesterol", "resting_blood_pressure", "fasting_blood_sugar"]
# Drop low correlation columns
high_corr_df = df.drop(low_corr, axis=1)
high_corr_df.head()

inputs_non_naive = high_corr_df.iloc[:, :-1]
target_col = df.columns[-1]
targets = high_corr_df[target_col]
inputs_non_naive.shape, targets.shape

inputs_non_naive = high_corr_df.iloc[:, :-1]
targets = high_corr_df[target_col]
inputs_non_naive.shape, targets.shape

X_train_non_naive, X_test_non_naive, y_train_non_naive, y_test_non_naive = train_test_split(inputs_non_naive, targets, test_size=0.40, random_state=3)

grid_search.fit(X_train_non_naive, y_train_non_naive)

best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Evaluate on test data
score_log = grid_search.score(X_test_non_naive, y_test_non_naive)

best_params, best_score, score_log

"""No improvement unfortunately. By filtering out features with low correlation"""

from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(x_train)
X_test_scaled = scaler.transform(x_test)

X_train_scaled = scaler.fit_transform(x_train.values)  # Convert DataFrame to NumPy array
X_test_scaled = scaler.transform(x_test.values)

# Step 6: Train SVC model without warning
svc_model = SVC(kernel='linear', random_state=42)
svc_model.fit(X_train_scaled, y_train)

# Step 7: Make Predictions
y_pred = svc_model.predict(X_test_scaled)
svc_model.score(X_test_scaled, y_test)
# accuracy = accuracy_score(y_test, y_pred)
# conf_matrix = confusion_matrix(y_test, y_pred)
# class_report = classification_report(y_test, y_pred)

# print(f"Accuracy: {accuracy:.2f}")
# print("Confusion Matrix:\n", conf_matrix)
# print("Classification Report:\n", class_report)

from sklearn import tree

model_tree = tree.DecisionTreeClassifier(max_depth=10)

model_tree.fit(x_train, y_train)
model_tree_score = model_tree.score(x_test, y_test)
model_tree_score

y_pred = model_tree.predict(x_test)

# Calculate the confusion matrix
cm = confusion_matrix(y_test, y_pred)
cm

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

random_forest = RandomForestClassifier()
random_forest.fit(x_train, y_train)
score_rf = random_forest.score(x_test, y_test)
score_rf

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Define the hyperparameters for tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Create a RandomForest model
random_forest = RandomForestClassifier()

# Apply GridSearchCV for hyperparameter tuning
grid_search_rf = GridSearchCV(random_forest, param_grid, cv=5, scoring='accuracy')
grid_search_rf.fit(x_train, y_train)

# Best parameters and score
best_params_rf = grid_search_rf.best_params_
best_score_rf = grid_search_rf.best_score_

# Evaluate on test data
score_rf = grid_search_rf.score(x_test, y_test)

best_params_rf, best_score_rf, score_rf

from xgboost import XGBClassifier

# Create an XGBoost model
xgb_model = XGBClassifier()

# Fit the model
xgb_model.fit(x_train, y_train)

# Evaluate the model
score_xgb = xgb_model.score(x_test, y_test)

score_xgb

from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0]
}

model = XGBClassifier()
grid_search = GridSearchCV(model, param_grid, scoring='accuracy', cv=5)
grid_search.fit(x_train, y_train)
print("Best parameters:", grid_search.best_params_)

best_params_xg = grid_search.best_params_
best_score_xg = grid_search.best_score_

# Evaluate on test data
score_rf = grid_search.score(x_test, y_test)
best_params_xg, best_score_xg, score_rf

